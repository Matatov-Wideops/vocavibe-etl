{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical Study\n",
    "* In this study, we collect voice recordings of people with Parkinson's disease (PD) and other neurodegenrative diseases like ataxia (AX), as well as healthy controls (HC).\n",
    "* The data collection is conducted using our app, **Vocapp**, which has two interfaces, for **users** (PD, HC), and for **samplers**.\n",
    "* The conductors of the study are **samplers**. Samplers can register new users. When registering a PD, the sampler also assesses the condition of the PD with clinical questionnaires like MDS-UPDRS, MoCA, etc., and documents everything in **Vocapp**. \n",
    "* After the registration, PDs will record themselves once every month, before taking medication and one hour afterwards. \n",
    "* HC and participants with other diseases will record themselves once without any further assessment or requirements.\n",
    "\n",
    "### Vocapp\n",
    "* The app has two interfaces, one for users and one for sampler. \n",
    "* When a user starts the recording exercises, the app starts a session. All the recordings and the self-report qnnrs. will be saved under this session. The filekey uploaded by a **user** has the following path: `{username}/{session}/{filename}`.\n",
    "* Sampler will upload qnnrs. files for a given user, but not under any session. The filekey of a file uploaded by a **sampler** will be: `{username}/{filename}`. This includes registration, medications, MDS-UPDRS, etc. \n",
    "\n",
    "### Alerts\n",
    "In order to collect the data effectively, we need an alerts system with the following alerts:\n",
    "* During session: if a user has stopped recording in the middle of the session, i.e. didn't upload a new file for 3 minutes, the app will send a Whatsapp msg with a reminder or with suggestions how to operate the app in case it is stuck.\n",
    "* Middle session: an hour after medication intake, send a reminder to record again.\n",
    "* Monthly reminder: send a monthly reminder to perform a recording session. Send a reminder on a weekly basis until the user has recorded. \n",
    "* Feedbak: we want to send a feedback as a bounty for PDs that have completed the full session (before + after medication). The feedback takes the recordings before medication, extracts some voice qualities into a dictionary, plots this dictionary, and sends it as a whatsapp msg.\n",
    "\n",
    "### Database\n",
    "* The dashboard is used to track and manage the clinical study. Thus, we extract all the metadata according to the following order:\n",
    "    * List all the files from the bucket (aws).\n",
    "    * Extract the metadata from the filekey name **only** (`bucket.csv`).\n",
    "    * Open the csv fils and extract additional metadata from them, including registration data and questionnaires (e.g. `raw.csv`).\n",
    "    * Files & Sessions:\n",
    "        * Update needed attributes for filekeys and merge with samplers and phone numbers databases.\n",
    "        * Match session to all filekeys (sampler's files do not have a session originally).\n",
    "        * Resolve sessions issues (mostly merge sessions).\n",
    "        * Propagate metadata to all filekeys.\n",
    "        * Save the full database (`all_files.csv`) and the sessions (`sessions.csv`).\n",
    "\n",
    "### Dashboard\n",
    "The dashbard uses the database to plot some figures:\n",
    "* Distribution of attributes \n",
    "* Patients registratino over time\n",
    "* Users per sampler\n",
    "* Number of full/part sessions vs. session number\n",
    "* Qnnrs. results\n",
    "* Broken (part) sessions\n",
    "* Users that need to record again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List all files from the bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- List all filekeys from the aws bucket.\n",
    "- Extract the metadata that is embedded in the file path **only**:\n",
    "    - filekey\n",
    "    - username\n",
    "    - Entity: PD, HC, Sampler (SA)\n",
    "    - time stamps\n",
    "    - pattern: RECORDING, RECORDING1, FOG, UPDRS3, etc.\n",
    "    - exercise: refers to RECORDING patterns, for other patterns, exercise = pattern.\n",
    "    - timing: \n",
    "        - pre: first recording part of the session, before medication.\n",
    "        - post: second recording part of the session, after medication.\n",
    "        - healthy: session of a healthy participant; one part only.\n",
    "    - onmed: did the patient *actually* take or didn't take the medication\n",
    "    - onoff: does the patient feel the effect of the medication (ON) or it has already worn off (OFF)\n",
    "- New filekeys are appended to the bucket csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_bucket(skip=True) -> None:\n",
    "    if (not exists(Settings.BUCKET_CSV)) or (skip==False):\n",
    "        bucket = pd.DataFrame(columns=Bucket.values())\n",
    "        processed_filekeys = []\n",
    "    else:\n",
    "        bucket = pd.read_csv(Settings.BUCKET_CSV, dtype=str)\n",
    "        processed_filekeys = bucket['filekey'].to_list()       \n",
    "    \n",
    "    filekeys = list_bucket()\n",
    "    filekeys = [f for f in filekeys if f not in processed_filekeys]\n",
    "\n",
    "    dfs = []\n",
    "    for filekey in tqdm(filekeys, desc=\"Adding new files to database\"):\n",
    "        df = pd.DataFrame(columns=Bucket.values())\n",
    "        df.loc[0, Bucket.FILEKEY] = filekey\n",
    "        filename = filekey.split('/')[-1]\n",
    "        for pattern in Patterns.values():\n",
    "            if re.match(pattern.value, filename):\n",
    "                username = filekey.split('/')[0]\n",
    "                df.loc[0, Bucket.USERNAME] = username\n",
    "                if username.startswith(\"hc_\"):\n",
    "                        df.loc[0, Bucket.ENTITY] = Entity.HC  # Ataxia will be resolved later\n",
    "                elif len(username)==40:\n",
    "                    df.loc[0, Bucket.ENTITY] = Entity.PD\n",
    "                else:\n",
    "                    df.loc[0, Bucket.ENTITY] = Entity.SA\n",
    "                \n",
    "                df.loc[0, Bucket.PATTERN] = pattern.name\n",
    "                df.loc[0, Bucket.EXERCISE] = pattern.name.lower()\n",
    "                if pattern.name not in [\"REGISTRATION0\", 'APKINSON']:\n",
    "                    df.loc[0, Bucket.DATE] = extract_from_filename(filekey, 'date')\n",
    "                    df.loc[0, Bucket.TIME] = extract_from_filename(filekey, 'time')\n",
    "                    df.loc[0, Bucket.DATETIME] = extract_from_filename(filekey, 'datetime')\n",
    "\n",
    "                if pattern.name!=\"UPDATE\":\n",
    "                    df.loc[0, Bucket.LANG] = extract_from_filename(filekey, 'language')\n",
    "                \n",
    "                if pattern.name in ['RECORDING', 'RECORDING1', 'FOG', 'SDQ', 'WOQ', 'UPDATE']:\n",
    "                    df.loc[0, Bucket.SESSION] = filekey.split('/')[1]\n",
    "                \n",
    "                if pattern.name in ['UPDRS', 'UPDRS3', 'UPDRS124']:\n",
    "                    df.loc[0, 'timing'] = extract_from_filename(filekey, 'timing')                    \n",
    "\n",
    "                if pattern.name=='RECORDING':\n",
    "                    df.loc[0, Bucket.EXERCISE] = extract_from_filename(filekey, 'exercise') # override\n",
    "                    df.loc[0, Bucket.TIMING] = extract_from_filename(filekey, 'timing')\n",
    "                    df.loc[0, Bucket.ONMED] = extract_from_filename(filekey, 'onmed')\n",
    "                    df.loc[0, Bucket.ONOFF] = extract_from_filename(filekey, 'onoff')\n",
    "                elif pattern.name=='RECORDING1':\n",
    "                    df.loc[0, Bucket.EXERCISE] = extract_from_filename(filekey, 'exercise') # override\n",
    "                    df.loc[0, Bucket.TIMING] = extract_from_filename(filekey, 'timing')\n",
    "                    df.loc[0, Bucket.ONMED] = OnMed.ONMED if filekey.endswith(\"_on\") else OnMed.NOTONMED\n",
    "                    df.loc[0, Bucket.ONOFF] = FeelOnOff.UNKNOWN\n",
    "                \n",
    "                dfs.append(df)\n",
    "                break\n",
    "    if dfs:\n",
    "        dfs = pd.concat(dfs, ignore_index=True)\n",
    "        bucket = pd.concat([bucket, dfs], ignore_index=True)\n",
    "        bucket.to_csv(Settings.BUCKET_CSV, index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch users login credentials from the VM (EC2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python code I run from my jupyter\n",
    "We did not keep the credentials on the VM from the beginning, so I had to concat the data with other logs to get all the phone numbers. This is the purpose of `combine_yahav_ec2()`. In later version, we can omit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def users_data():\n",
    "    print(\"\\nDownloading credentials from ec2...\")\n",
    "    os_type = get_os()\n",
    "    if os_type == \"Linux\" or os_type == \"Darwin\":\n",
    "        run_shell_command(\"chmod +x src/download_users.sh\")\n",
    "        run_shell_command(\"./src/download_users.sh\")\n",
    "    elif os_type == \"Windows\":\n",
    "        run_windows_shell_command(\"./src/download_users_windows.ps1\")\n",
    "    else:\n",
    "        print(f\"Unsupported OS: {os_type}\")\n",
    "        raise SystemExit\n",
    "    \n",
    "    combine_yahav_ec2()\n",
    "    healthy_ec2()\n",
    "\n",
    "\n",
    "\n",
    "def combine_yahav_ec2() -> None:\n",
    "    pd_yahav = pd.read_csv(Settings.USERS_YAHAV_CSV, dtype=str, index_col=0)\n",
    "    pd_yahav[ExtraCols.PASSWORD.value] = np.nan\n",
    "    pd_ec2 = pd.read_csv(Settings.USERSPD, dtype=str)\n",
    "    pd_ec2.columns = [ExtraCols.USER_PHONE.value, ExtraCols.PASSWORD.value]\n",
    "    pd_ec2['username'] = pd_ec2[ExtraCols.USER_PHONE.value].apply(hash_phone_number)\n",
    "    combo = pd.concat([pd_yahav, pd_ec2], ignore_index=True)\n",
    "\n",
    "    phonesNpasswords = pd.read_csv('resources/passwords.csv', dtype=str)\n",
    "    # Merge the dataframes on ExtraCols.USER_PHONE.value with an outer join to ensure all users are included\n",
    "    combined_df = pd.merge(combo, phonesNpasswords, on=ExtraCols.USER_PHONE.value, how='outer', suffixes=('_users', '_passwords'))\n",
    "\n",
    "    # Fill missing passwords in users_df with passwords from passwords_df\n",
    "    combined_df[ExtraCols.PASSWORD.value] = combined_df['password_users'].combine_first(combined_df['password_passwords'])\n",
    "\n",
    "    # Drop the now redundant columns\n",
    "    combined_df.drop(columns=['password_users', 'password_passwords'], inplace=True)\n",
    "    combined_df = combined_df.drop_duplicates(subset=ExtraCols.USER_PHONE.value, keep='last', ignore_index=True)\n",
    "    combined_df.to_csv(Settings.USERS_EC2_CSV)\n",
    "\n",
    "\n",
    "\n",
    "def healthy_ec2() -> None:\n",
    "    hc_ec2 = pd.read_csv(Settings.USERSHC, dtype=str)\n",
    "    hc_ec2.columns = [ExtraCols.USER_PHONE.value, ExtraCols.PASSWORD.value]\n",
    "    hc_ec2[Bucket.USERNAME] = hc_ec2[ExtraCols.USER_PHONE.value].apply(hash_phone_number)\n",
    "    hc_ec2[Bucket.USERNAME] = \"hc_\" + hc_ec2[Bucket.USERNAME]\n",
    "\n",
    "    tocat = pd.read_csv(Settings.HC_PHONES_CSV, dtype=str, index_col=0)\n",
    "    tocat[ExtraCols.PASSWORD.value] = np.nan\n",
    "    hc_ec2 = pd.concat([hc_ec2, tocat], ignore_index=True)\n",
    "    hc_ec2 = hc_ec2.drop_duplicates(subset=ExtraCols.USER_PHONE.value, keep='last', ignore_index=True)\n",
    "    hc_ec2.to_csv(Settings.HC_EC2_CSV)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shell script to access the VM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "#!/bin/bash\n",
    "\n",
    "# Define variables\n",
    "PEM_FILE=\"../build-key.pem\"\n",
    "EC2_USER=\"ec2-user\"\n",
    "EC2_HOST=\"ec2-3-83-206-91.compute-1.amazonaws.com\"\n",
    "CONTAINER_NAME=\"vocabe\"\n",
    "REMOTE_FILE_PATH1=\"/data/.usershc.csv\"\n",
    "EC2_LOCAL_PATH1=\"/home/ec2-user/.usershc.csv\"\n",
    "LOCAL_FILE_PATH1=\"resources/usershc.csv\"\n",
    "REMOTE_FILE_PATH2=\"/data/.userspd.csv\"\n",
    "EC2_LOCAL_PATH2=\"/home/ec2-user/.userspd.csv\"\n",
    "LOCAL_FILE_PATH2=\"resources/userspd.csv\"\n",
    "\n",
    "# Log into AWS EC2 and copy the file from the Docker container to the EC2 instance's home directory\n",
    "ssh -i $PEM_FILE $EC2_USER@$EC2_HOST << EOF\n",
    "  sudo docker cp $CONTAINER_NAME:$REMOTE_FILE_PATH1 $EC2_LOCAL_PATH1\n",
    "  sudo docker cp $CONTAINER_NAME:$REMOTE_FILE_PATH2 $EC2_LOCAL_PATH2\n",
    "EOF\n",
    "\n",
    "# Check if the SSH command was successful\n",
    "if [ $? -eq 0 ]; then\n",
    "  echo \"File copied from container to EC2 instance successfully.\"\n",
    "else\n",
    "  echo \"Failed to copy file from container to EC2 instance.\"\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "# Download the file from the EC2 instance to the local machine\n",
    "scp -i $PEM_FILE $EC2_USER@$EC2_HOST:$EC2_LOCAL_PATH1 $LOCAL_FILE_PATH1\n",
    "scp -i $PEM_FILE $EC2_USER@$EC2_HOST:$EC2_LOCAL_PATH2 $LOCAL_FILE_PATH2\n",
    "\n",
    "# Check if the SCP command was successful\n",
    "if [ $? -eq 0 ]; then\n",
    "  echo \"File downloaded to local machine successfully.\"\n",
    "else\n",
    "  echo \"Failed to download file to local machine.\"\n",
    "  exit 1\n",
    "fi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract raw data\n",
    "Open csv files to get the data of:\n",
    "- Participants at registration\n",
    "- Data updates\n",
    "- Answers to questionnaires\n",
    "- Medications list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_raw_data(skip=True, print_filekey=False):\n",
    "    pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "    def check_exist_and_return(filepath: str, skip=skip) -> pd.DataFrame:\n",
    "        if (not exists(filepath)) or (skip==False):\n",
    "            return pd.DataFrame()\n",
    "        else:\n",
    "            return pd.read_csv(filepath, dtype=str)\n",
    "    \n",
    "    def put_filekey_first(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if 'filekey' in df.columns:\n",
    "            cols = df.columns.tolist()\n",
    "            cols.remove('filekey')\n",
    "            df = df[['filekey'] + cols]\n",
    "        else:\n",
    "            print(\"The DataFrame does not contain a 'filekey' column.\")\n",
    "        return df\n",
    "\n",
    "    bucket = pd.read_csv(Settings.BUCKET_CSV, dtype=str)\n",
    "    raw = pd.read_csv(Settings.RAW_CSV, dtype=str) if exists(Settings.RAW_CSV) else pd.DataFrame()\n",
    "    if exists(Settings.RAW_CSV):\n",
    "        new_filekeys = bucket.loc[~bucket[Bucket.FILEKEY].isin(raw[Bucket.FILEKEY])].copy()\n",
    "    else:\n",
    "        new_filekeys = bucket.copy()\n",
    "    \n",
    "\n",
    "    updrs = check_exist_and_return(Settings.UPDRS_CSV)\n",
    "    moca = check_exist_and_return(Settings.MOCA_CSV)\n",
    "    pdq8 = check_exist_and_return(Settings.PDQ8_CSV)\n",
    "    fog = check_exist_and_return(Settings.FOG_CSV)\n",
    "    sdq = check_exist_and_return(Settings.SDQ_CSV)\n",
    "    woq = check_exist_and_return(Settings.WOQ_CSV)\n",
    "    registration = check_exist_and_return(Settings.REGISTRATION_CSV)\n",
    "    update = check_exist_and_return(Settings.UPDATE_CSV)\n",
    "    medications = check_exist_and_return(Settings.MEDICATION_CSV)\n",
    "\n",
    "    for ii,row in tqdm(new_filekeys.iterrows(), desc=\"Extracting raw data\", total=len(new_filekeys)):\n",
    "        filekey = row[Bucket.FILEKEY]\n",
    "        pattern = row[Bucket.PATTERN]\n",
    "        if print_filekey:\n",
    "            print(filekey)\n",
    "        if pattern in [\"UPDRS\", \"UPDRS3\", \"UPDRS124\"]:\n",
    "            if Qnnrs.UPDRS1.value not in row or pd.isna(row[Qnnrs.UPDRS1.value]) or row[Qnnrs.UPDRS1.value]=='':\n",
    "                df = download_csv_to_df(filekey)\n",
    "                new_filekeys.loc[ii, Qnnrs.UPDRS1] = df.loc[0, UPDRS.updrs1.value].astype(int).sum()\n",
    "                new_filekeys.loc[ii, Qnnrs.UPDRS2] = df.loc[0, UPDRS.updrs2.value].astype(int).sum()\n",
    "                new_filekeys.loc[ii, Qnnrs.UPDRS3] = df.loc[0, UPDRS.updrs3.value].astype(int).sum()\n",
    "                new_filekeys.loc[ii, Qnnrs.UPDRS4] = df.loc[0, UPDRS.updrs4.value].astype(int).sum()\n",
    "                new_filekeys.loc[ii, Qnnrs.HY] = df.loc[0, UPDRS.hy.value]\n",
    "                new_filekeys.loc[ii, Bucket.SAMPLER] = df.loc[0, Bucket.SAMPLER] if Bucket.SAMPLER in df else pd.NaT\n",
    "                if updrs.empty or (filekey not in updrs[Bucket.FILEKEY].tolist()):\n",
    "                    df[Bucket.FILEKEY] = filekey\n",
    "                    df = put_filekey_first(df)\n",
    "                    updrs = pd.concat([updrs, df], ignore_index=True)\n",
    "        \n",
    "        if pattern==\"MOCA\":\n",
    "            if Qnnrs.MOCA.value not in row or pd.isna(row[Qnnrs.MOCA.value]) or row[Qnnrs.MOCA.value]=='':\n",
    "                df = download_csv_to_df(filekey)\n",
    "                df = df.replace({\"True\": 1, \"False\": 0})\n",
    "                new_filekeys.loc[ii, Qnnrs.MOCA] = df.loc[0, MoCA.moca.value].astype(int).sum()\n",
    "                new_filekeys.loc[ii, Bucket.SAMPLER] = df.loc[0, Bucket.SAMPLER] if Bucket.SAMPLER in df else pd.NaT\n",
    "                if moca.empty or (filekey not in moca[Bucket.FILEKEY].tolist()):\n",
    "                    df[Bucket.FILEKEY] = filekey\n",
    "                    df = put_filekey_first(df)\n",
    "                    moca = pd.concat([moca, df], ignore_index=True)\n",
    "        \n",
    "        if pattern==\"PDQ8\":\n",
    "            if Qnnrs.PDQ8.value not in row or pd.isna(row[Qnnrs.PDQ8.value]) or row[Qnnrs.PDQ8.value]=='':\n",
    "                df = download_csv_to_df(filekey)\n",
    "                new_filekeys.loc[ii, Qnnrs.PDQ8] = df.loc[0, PDQ8.pdq8.value].astype(int).sum()\n",
    "                new_filekeys.loc[ii, Bucket.SAMPLER] = df.loc[0, Bucket.SAMPLER] if Bucket.SAMPLER in df else pd.NaT\n",
    "                if pdq8.empty or (filekey not in pdq8[Bucket.FILEKEY].tolist()):\n",
    "                    df[Bucket.FILEKEY] = filekey\n",
    "                    df = put_filekey_first(df)\n",
    "                    pdq8 = pd.concat([pdq8, df], ignore_index=True)\n",
    "        \n",
    "        if pattern==\"FOG\":\n",
    "            if Qnnrs.FOG.value not in row or pd.isna(row[Qnnrs.FOG.value]) or row[Qnnrs.FOG.value]=='':\n",
    "                df = download_csv_to_df(filekey)\n",
    "                new_filekeys.loc[ii, Qnnrs.FOG] = df.loc[0, FOG.fog.value].astype(int).sum()\n",
    "                if fog.empty or (filekey not in fog[Bucket.FILEKEY].tolist()):\n",
    "                    df[Bucket.FILEKEY] = filekey\n",
    "                    df = put_filekey_first(df)\n",
    "                    fog = pd.concat([fog, df], ignore_index=True)\n",
    "        \n",
    "        if pattern==\"SDQ\":\n",
    "            if Qnnrs.SDQ.value not in row or pd.isna(row[Qnnrs.SDQ.value]) or row[Qnnrs.SDQ.value]=='':\n",
    "                df = download_csv_to_df(filekey)\n",
    "                df = df.replace({\"True\": 1, \"False\": 0})\n",
    "                score = df.loc[0, SDQ.sdq.value[:-1]].astype(int).sum()\n",
    "                respiratory = 2.5 if df.loc[0, SDQ.sdq.value[-1]]==\"True\" else 0.5\n",
    "                score += respiratory\n",
    "                new_filekeys.loc[ii, Qnnrs.SDQ] = score\n",
    "                if sdq.empty or (filekey not in sdq[Bucket.FILEKEY].tolist()):\n",
    "                    df[Bucket.FILEKEY] = filekey\n",
    "                    df = put_filekey_first(df)\n",
    "                    sdq = pd.concat([sdq, df], ignore_index=True)\n",
    "        \n",
    "        if pattern==\"WOQ\":\n",
    "            if Qnnrs.WOQ_PRE.value not in row or pd.isna(row[Qnnrs.WOQ_PRE.value]) or row[Qnnrs.WOQ_PRE.value]=='':\n",
    "                df = download_csv_to_df(filekey)\n",
    "                df = df.replace({\"True\": 1, \"False\": 0})\n",
    "                try:\n",
    "                    new_filekeys.loc[ii, Qnnrs.WOQ_PRE] = df.loc[0, WOQ.pre.value].astype(int).sum()\n",
    "                    new_filekeys.loc[ii, Qnnrs.WOQ_POST] = df.loc[0, WOQ.pre.value].astype(int).sum() - df.loc[0, WOQ.post.value].astype(int).sum()\n",
    "                except:\n",
    "                    new_filekeys.loc[ii, Qnnrs.WOQ_PRE] = -1\n",
    "                    new_filekeys.loc[ii, Qnnrs.WOQ_POST] = -1\n",
    "                if woq.empty or (filekey not in woq[Bucket.FILEKEY].tolist()):\n",
    "                    df[Bucket.FILEKEY] = filekey\n",
    "                    df = put_filekey_first(df)\n",
    "                    woq = pd.concat([woq, df], ignore_index=True)\n",
    "\n",
    "        if pattern==\"REGISTRATION\":\n",
    "            if Registration.BIRTHDATE.value not in row or pd.isna(row[Registration.BIRTHDATE.value]) or row[Registration.BIRTHDATE.value]=='':\n",
    "                df = download_csv_to_df(filekey)\n",
    "                for col in Registration.values():\n",
    "                    if col in df:\n",
    "                        new_filekeys.loc[ii, col] = str(df.loc[0, col])\n",
    "                if registration.empty or (filekey not in registration[Bucket.FILEKEY].tolist()):\n",
    "                    df[Bucket.FILEKEY] = filekey\n",
    "                    df = put_filekey_first(df)\n",
    "                    registration = pd.concat([registration, df], ignore_index=True)\n",
    "    \n",
    "        if pattern==\"UPDATE\":\n",
    "            if Update.IS_DBS.value not in row or pd.isna(row[Update.IS_DBS.value]) or row[Update.IS_DBS.value]=='':\n",
    "                df = download_csv_to_df(filekey)\n",
    "                for col in Update.values():\n",
    "                    if col in df:\n",
    "                        new_filekeys.loc[ii, col] = str(df.loc[0, col])\n",
    "                if update.empty or (filekey not in update[Bucket.FILEKEY].tolist()):\n",
    "                    df = download_csv_to_df(filekey)\n",
    "                    df[Bucket.FILEKEY] = filekey\n",
    "                    df = put_filekey_first(df)\n",
    "                    update = pd.concat([update, df], ignore_index=True)\n",
    "\n",
    "        if pattern==\"MEDICATIONS\":\n",
    "            if medications.empty or (filekey not in medications[Bucket.FILEKEY].tolist()):\n",
    "                df = download_csv_to_df(filekey)\n",
    "                df[Bucket.FILEKEY] = filekey\n",
    "                df = put_filekey_first(df)\n",
    "                medications = pd.concat([medications, df.astype(str)], ignore_index=True)\n",
    "\n",
    "        \n",
    "    updrs.to_csv(Settings.UPDRS_CSV, index=False)\n",
    "    moca.to_csv(Settings.MOCA_CSV, index=False)\n",
    "    pdq8.to_csv(Settings.PDQ8_CSV, index=False)\n",
    "    fog.to_csv(Settings.FOG_CSV, index=False)\n",
    "    sdq.to_csv(Settings.SDQ_CSV, index=False)\n",
    "    woq.to_csv(Settings.WOQ_CSV, index=False)\n",
    "    update.to_csv(Settings.UPDATE_CSV, index=False)\n",
    "    registration.to_csv(Settings.REGISTRATION_CSV, index=False)\n",
    "    medications.to_csv(Settings.MEDICATION_CSV, index=False)\n",
    "\n",
    "    raw = pd.concat([raw, new_filekeys], ignore_index=True)\n",
    "    raw = raw.sort_values(by=['date', 'username', 'time'], ascending=False, ignore_index=True)\n",
    "    raw.to_csv(Settings.RAW_CSV, index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Get all files\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `change_columns()`: update values of attributes (columns) for specific filekeys.\n",
    "* `resolve()`: remove unneeded filekeys, update attributes for old patterns or filenames.\n",
    "* `add_patient_phone()`: merge sampler's name (from the registration csv) with samplers phones csv.\n",
    "* `add_patient_phone()`: use the data from the VM to merge the phone numbers of the patients (users).\n",
    "* `add_session_to_all()`: find the closest session (in time) for the sampler's files.\n",
    "* `resolve_sessions()`: resolve sessions issues; mostly merging sessions.\n",
    "* `propagate_values()`: propagate the data from the csv files to all filekeys.\n",
    "* Create `all_files.csv`, `sessions.csv`, and `all_users.csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_all_files():\n",
    "    print(\"Arranging data ...\", end=' ')\n",
    "    df = pd.read_csv(Settings.RAW_CSV, dtype=str)\n",
    "    df = change_columns(df)\n",
    "    df = resolve(df)\n",
    "\n",
    "    df = add_sampler_phone(df)\n",
    "    df = add_patient_phone(df)\n",
    "    df = add_caregiver_phone(df)\n",
    "\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], format=Settings.DATETIME, errors='coerce')\n",
    "    df = add_session_to_all(df)\n",
    "\n",
    "    real_sessions = df.copy()\n",
    "    real_sessions = add_session_number(real_sessions)\n",
    "    real_sessions = add_sampler_to_HC(real_sessions)\n",
    "\n",
    "    df = resolve_sessions(df)\n",
    "    df = add_session_number(df)\n",
    "    df = remove_qnnrs_duplicates(df)\n",
    "    df = add_sampler_to_HC(df)\n",
    "    print(\"Done!\")\n",
    "\n",
    "    df = propagate_values(df)\n",
    "    df = add_age(df)\n",
    "    df = add_updrs_columns(df)\n",
    "    df = df.sort_values(by=['date', 'username', 'time'], ascending=False)\n",
    "    df.to_csv(Settings.ALL_FILES, index=False)\n",
    "    \n",
    "    sessions = get_sessions(df)\n",
    "    sessions.to_csv(Settings.SESSIONS, index=False)\n",
    "    real_sessions = get_sessions(real_sessions)\n",
    "    real_sessions.to_csv(Settings.REAL_SESSIONS, index=False)\n",
    "\n",
    "    all_users = get_all_users(df)\n",
    "    all_users.to_csv(Settings.ALL_USERS, index=False)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pie plots\n",
    "![](pies.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users over time\n",
    "![](overtime.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users per sampler\n",
    "![How many users (PD/HC) each sampler sampled?](userpersampler.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of sessions vs. session number\n",
    "![](sessions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questionnaires results\n",
    "![](qnnrs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncompleted session (broken sessions)\n",
    "![](broken.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users that need to record their next session\n",
    "![](password.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
