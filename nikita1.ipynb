{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical Study\n",
    "* In this study, we collect voice recordings of people with Parkinson's disease (PD) and other neurodegenrative diseases like ataxia (AX), as well as healthy controls (HC).\n",
    "* The data collection is conducted using our app, **Vocapp**, which has two interfaces, for **users** (PD, HC), and for **samplers**.\n",
    "* The conductors of the study are **samplers**. Samplers can register new users. When registering a PD, the sampler also assesses the condition of the PD with clinical questionnaires like MDS-UPDRS, MoCA, etc., and documents everything in **Vocapp**. \n",
    "* After the registration, PDs will record themselves once every month, before taking medication and one hour afterwards. \n",
    "* HC and participants with other diseases will record themselves once without any further assessment or requirements.\n",
    "\n",
    "### Vocapp\n",
    "* The app has two interfaces, one for users and one for sampler. \n",
    "* When a user starts the recording exercises, the app starts a session. All the recordings and the self-report qnnrs. will be saved under this session. The filekey uploaded by a **user** has the following path: `{username}/{session}/{filename}`.\n",
    "* Sampler will upload qnnrs. files for a given user, but not under any session. The filekey of a file uploaded by a **sampler** will be: `{username}/{filename}`. This includes registration, medications, MDS-UPDRS, etc. \n",
    "\n",
    "### Alerts\n",
    "In order to collect the data effectively, we need an alerts system with the following alerts:\n",
    "* During session: if a user has stopped recording in the middle of the session, i.e. didn't upload a new file for 3 minutes, the app will send a Whatsapp msg with a reminder or with suggestions how to operate the app in case it is stuck.\n",
    "* Middle session: an hour after medication intake, send a reminder to record again.\n",
    "* Monthly reminder: send a monthly reminder to perform a recording session. Send a reminder on a weekly basis until the user has recorded. \n",
    "* Feedbak: we want to send a feedback as a bounty for PDs that have completed the full session (before + after medication). The feedback takes the recordings before medication, extracts some voice qualities into a dictionary, plots this dictionary, and sends it as a whatsapp msg.\n",
    "\n",
    "### Database\n",
    "* The dashboard is used to track and manage the clinical study. Thus, we extract all the metadata according to the following order:\n",
    "    * List all the files from the bucket (aws).\n",
    "    * Extract the metadata from the filekey name **only** (`bucket.csv`).\n",
    "    * Open the csv fils and extract additional metadata from them, including registration data and questionnaires (e.g. `raw.csv`).\n",
    "    * Files & Sessions:\n",
    "        * Update needed attributes for filekeys and merge with samplers and phone numbers databases.\n",
    "        * Match session to all filekeys (sampler's files do not have a session originally).\n",
    "        * Resolve sessions issues (mostly merge sessions).\n",
    "        * Propagate metadata to all filekeys.\n",
    "        * Save the full database (`all_files.csv`) and the sessions (`sessions.csv`).\n",
    "\n",
    "### Dashboard\n",
    "The dashbard uses the database to plot some figures:\n",
    "* Distribution of attributes \n",
    "* Patients registratino over time\n",
    "* Users per sampler\n",
    "* Number of full/part sessions vs. session number\n",
    "* Qnnrs. results\n",
    "* Broken (part) sessions\n",
    "* Users that need to record again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List all files from the bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- List all filekeys from the aws bucket.\n",
    "- Extract the metadata that is embedded in the file path **only**:\n",
    "    - filekey\n",
    "    - username\n",
    "    - Entity: PD, HC, Sampler (SA)\n",
    "    - time stamps\n",
    "    - pattern: RECORDING, RECORDING1, FOG, UPDRS3, etc.\n",
    "    - exercise: refers to RECORDING patterns, for other patterns, exercise = pattern.\n",
    "    - timing: \n",
    "        - pre: first recording part of the session, before medication.\n",
    "        - post: second recording part of the session, after medication.\n",
    "        - healthy: session of a healthy participant; one part only.\n",
    "    - onmed: did the patient *actually* take or didn't take the medication\n",
    "    - onoff: does the patient feel the effect of the medication (ON) or it has already worn off (OFF)\n",
    "- New filekeys are appended to the bucket csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_bucket(skip=True) -> None:\n",
    "    if (not exists(Settings.BUCKET_CSV)) or (skip==False):\n",
    "        bucket = pd.DataFrame(columns=Bucket.values())\n",
    "        processed_filekeys = []\n",
    "    else:\n",
    "        bucket = pd.read_csv(Settings.BUCKET_CSV, dtype=str)\n",
    "        processed_filekeys = bucket['filekey'].to_list()       \n",
    "    \n",
    "    filekeys = list_bucket()\n",
    "    filekeys = [f for f in filekeys if f not in processed_filekeys]\n",
    "\n",
    "    dfs = []\n",
    "    for filekey in tqdm(filekeys, desc=\"Adding new files to database\"):\n",
    "        df = pd.DataFrame(columns=Bucket.values())\n",
    "        df.loc[0, Bucket.FILEKEY] = filekey\n",
    "        filename = filekey.split('/')[-1]\n",
    "        for pattern in Patterns.values():\n",
    "            if re.match(pattern.value, filename):\n",
    "                username = filekey.split('/')[0]\n",
    "                df.loc[0, Bucket.USERNAME] = username\n",
    "                if username.startswith(\"hc_\"):\n",
    "                        df.loc[0, Bucket.ENTITY] = Entity.HC  # Ataxia will be resolved later\n",
    "                elif len(username)==40:\n",
    "                    df.loc[0, Bucket.ENTITY] = Entity.PD\n",
    "                else:\n",
    "                    df.loc[0, Bucket.ENTITY] = Entity.SA\n",
    "                \n",
    "                df.loc[0, Bucket.PATTERN] = pattern.name\n",
    "                df.loc[0, Bucket.EXERCISE] = pattern.name.lower()\n",
    "                if pattern.name not in [\"REGISTRATION0\", 'APKINSON']:\n",
    "                    df.loc[0, Bucket.DATE] = extract_from_filename(filekey, 'date')\n",
    "                    df.loc[0, Bucket.TIME] = extract_from_filename(filekey, 'time')\n",
    "                    df.loc[0, Bucket.DATETIME] = extract_from_filename(filekey, 'datetime')\n",
    "\n",
    "                if pattern.name!=\"UPDATE\":\n",
    "                    df.loc[0, Bucket.LANG] = extract_from_filename(filekey, 'language')\n",
    "                \n",
    "                if pattern.name in ['RECORDING', 'RECORDING1', 'FOG', 'SDQ', 'WOQ', 'UPDATE']:\n",
    "                    df.loc[0, Bucket.SESSION] = filekey.split('/')[1]\n",
    "                \n",
    "                if pattern.name in ['UPDRS', 'UPDRS3', 'UPDRS124']:\n",
    "                    df.loc[0, 'timing'] = extract_from_filename(filekey, 'timing')                    \n",
    "\n",
    "                if pattern.name=='RECORDING':\n",
    "                    df.loc[0, Bucket.EXERCISE] = extract_from_filename(filekey, 'exercise') # override\n",
    "                    df.loc[0, Bucket.TIMING] = extract_from_filename(filekey, 'timing')\n",
    "                    df.loc[0, Bucket.ONMED] = extract_from_filename(filekey, 'onmed')\n",
    "                    df.loc[0, Bucket.ONOFF] = extract_from_filename(filekey, 'onoff')\n",
    "                elif pattern.name=='RECORDING1':\n",
    "                    df.loc[0, Bucket.EXERCISE] = extract_from_filename(filekey, 'exercise') # override\n",
    "                    df.loc[0, Bucket.TIMING] = extract_from_filename(filekey, 'timing')\n",
    "                    df.loc[0, Bucket.ONMED] = OnMed.ONMED if filekey.endswith(\"_on\") else OnMed.NOTONMED\n",
    "                    df.loc[0, Bucket.ONOFF] = FeelOnOff.UNKNOWN\n",
    "                \n",
    "                dfs.append(df)\n",
    "                break\n",
    "    if dfs:\n",
    "        dfs = pd.concat(dfs, ignore_index=True)\n",
    "        bucket = pd.concat([bucket, dfs], ignore_index=True)\n",
    "        bucket.to_csv(Settings.BUCKET_CSV, index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch users login credentials from the VM (EC2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python code I run from my jupyter\n",
    "We did not keep the credentials on the VM from the beginning, so I had to concat the data with other logs to get all the phone numbers. This is the purpose of `combine_yahav_ec2()`. In later version, we can omit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def users_data():\n",
    "    print(\"\\nDownloading credentials from ec2...\")\n",
    "    os_type = get_os()\n",
    "    if os_type == \"Linux\" or os_type == \"Darwin\":\n",
    "        run_shell_command(\"chmod +x src/download_users.sh\")\n",
    "        run_shell_command(\"./src/download_users.sh\")\n",
    "    elif os_type == \"Windows\":\n",
    "        run_windows_shell_command(\"./src/download_users_windows.ps1\")\n",
    "    else:\n",
    "        print(f\"Unsupported OS: {os_type}\")\n",
    "        raise SystemExit\n",
    "    \n",
    "    combine_yahav_ec2()\n",
    "    healthy_ec2()\n",
    "\n",
    "\n",
    "\n",
    "def combine_yahav_ec2() -> None:\n",
    "    pd_yahav = pd.read_csv(Settings.USERS_YAHAV_CSV, dtype=str, index_col=0)\n",
    "    pd_yahav[ExtraCols.PASSWORD.value] = np.nan\n",
    "    pd_ec2 = pd.read_csv(Settings.USERSPD, dtype=str)\n",
    "    pd_ec2.columns = [ExtraCols.USER_PHONE.value, ExtraCols.PASSWORD.value]\n",
    "    pd_ec2['username'] = pd_ec2[ExtraCols.USER_PHONE.value].apply(hash_phone_number)\n",
    "    combo = pd.concat([pd_yahav, pd_ec2], ignore_index=True)\n",
    "\n",
    "    phonesNpasswords = pd.read_csv('resources/passwords.csv', dtype=str)\n",
    "    # Merge the dataframes on ExtraCols.USER_PHONE.value with an outer join to ensure all users are included\n",
    "    combined_df = pd.merge(combo, phonesNpasswords, on=ExtraCols.USER_PHONE.value, how='outer', suffixes=('_users', '_passwords'))\n",
    "\n",
    "    # Fill missing passwords in users_df with passwords from passwords_df\n",
    "    combined_df[ExtraCols.PASSWORD.value] = combined_df['password_users'].combine_first(combined_df['password_passwords'])\n",
    "\n",
    "    # Drop the now redundant columns\n",
    "    combined_df.drop(columns=['password_users', 'password_passwords'], inplace=True)\n",
    "    combined_df = combined_df.drop_duplicates(subset=ExtraCols.USER_PHONE.value, keep='last', ignore_index=True)\n",
    "    combined_df.to_csv(Settings.USERS_EC2_CSV)\n",
    "\n",
    "\n",
    "\n",
    "def healthy_ec2() -> None:\n",
    "    hc_ec2 = pd.read_csv(Settings.USERSHC, dtype=str)\n",
    "    hc_ec2.columns = [ExtraCols.USER_PHONE.value, ExtraCols.PASSWORD.value]\n",
    "    hc_ec2[Bucket.USERNAME] = hc_ec2[ExtraCols.USER_PHONE.value].apply(hash_phone_number)\n",
    "    hc_ec2[Bucket.USERNAME] = \"hc_\" + hc_ec2[Bucket.USERNAME]\n",
    "\n",
    "    tocat = pd.read_csv(Settings.HC_PHONES_CSV, dtype=str, index_col=0)\n",
    "    tocat[ExtraCols.PASSWORD.value] = np.nan\n",
    "    hc_ec2 = pd.concat([hc_ec2, tocat], ignore_index=True)\n",
    "    hc_ec2 = hc_ec2.drop_duplicates(subset=ExtraCols.USER_PHONE.value, keep='last', ignore_index=True)\n",
    "    hc_ec2.to_csv(Settings.HC_EC2_CSV)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shell script to access the VM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "#!/bin/bash\n",
    "\n",
    "# Define variables\n",
    "PEM_FILE=\"../build-key.pem\"\n",
    "EC2_USER=\"ec2-user\"\n",
    "EC2_HOST=\"ec2-3-83-206-91.compute-1.amazonaws.com\"\n",
    "CONTAINER_NAME=\"vocabe\"\n",
    "REMOTE_FILE_PATH1=\"/data/.usershc.csv\"\n",
    "EC2_LOCAL_PATH1=\"/home/ec2-user/.usershc.csv\"\n",
    "LOCAL_FILE_PATH1=\"resources/usershc.csv\"\n",
    "REMOTE_FILE_PATH2=\"/data/.userspd.csv\"\n",
    "EC2_LOCAL_PATH2=\"/home/ec2-user/.userspd.csv\"\n",
    "LOCAL_FILE_PATH2=\"resources/userspd.csv\"\n",
    "\n",
    "# Log into AWS EC2 and copy the file from the Docker container to the EC2 instance's home directory\n",
    "ssh -i $PEM_FILE $EC2_USER@$EC2_HOST << EOF\n",
    "  sudo docker cp $CONTAINER_NAME:$REMOTE_FILE_PATH1 $EC2_LOCAL_PATH1\n",
    "  sudo docker cp $CONTAINER_NAME:$REMOTE_FILE_PATH2 $EC2_LOCAL_PATH2\n",
    "EOF\n",
    "\n",
    "# Check if the SSH command was successful\n",
    "if [ $? -eq 0 ]; then\n",
    "  echo \"File copied from container to EC2 instance successfully.\"\n",
    "else\n",
    "  echo \"Failed to copy file from container to EC2 instance.\"\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "# Download the file from the EC2 instance to the local machine\n",
    "scp -i $PEM_FILE $EC2_USER@$EC2_HOST:$EC2_LOCAL_PATH1 $LOCAL_FILE_PATH1\n",
    "scp -i $PEM_FILE $EC2_USER@$EC2_HOST:$EC2_LOCAL_PATH2 $LOCAL_FILE_PATH2\n",
    "\n",
    "# Check if the SCP command was successful\n",
    "if [ $? -eq 0 ]; then\n",
    "  echo \"File downloaded to local machine successfully.\"\n",
    "else\n",
    "  echo \"Failed to download file to local machine.\"\n",
    "  exit 1\n",
    "fi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract raw data\n",
    "Open csv files to get the data of:\n",
    "- Participants at registration\n",
    "- Data updates\n",
    "- Answers to questionnaires\n",
    "- Medications list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_raw_data(skip=True, print_filekey=False):\n",
    "    pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "    def check_exist_and_return(filepath: str, skip=skip) -> pd.DataFrame:\n",
    "        if (not exists(filepath)) or (skip==False):\n",
    "            return pd.DataFrame()\n",
    "        else:\n",
    "            return pd.read_csv(filepath, dtype=str)\n",
    "    \n",
    "    def put_filekey_first(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if 'filekey' in df.columns:\n",
    "            cols = df.columns.tolist()\n",
    "            cols.remove('filekey')\n",
    "            df = df[['filekey'] + cols]\n",
    "        else:\n",
    "            print(\"The DataFrame does not contain a 'filekey' column.\")\n",
    "        return df\n",
    "\n",
    "    bucket = pd.read_csv(Settings.BUCKET_CSV, dtype=str)\n",
    "    raw = pd.read_csv(Settings.RAW_CSV, dtype=str) if exists(Settings.RAW_CSV) else pd.DataFrame()\n",
    "    if exists(Settings.RAW_CSV):\n",
    "        new_filekeys = bucket.loc[~bucket[Bucket.FILEKEY].isin(raw[Bucket.FILEKEY])].copy()\n",
    "    else:\n",
    "        new_filekeys = bucket.copy()\n",
    "    \n",
    "\n",
    "    updrs = check_exist_and_return(Settings.UPDRS_CSV)\n",
    "    moca = check_exist_and_return(Settings.MOCA_CSV)\n",
    "    pdq8 = check_exist_and_return(Settings.PDQ8_CSV)\n",
    "    fog = check_exist_and_return(Settings.FOG_CSV)\n",
    "    sdq = check_exist_and_return(Settings.SDQ_CSV)\n",
    "    woq = check_exist_and_return(Settings.WOQ_CSV)\n",
    "    registration = check_exist_and_return(Settings.REGISTRATION_CSV)\n",
    "    update = check_exist_and_return(Settings.UPDATE_CSV)\n",
    "    medications = check_exist_and_return(Settings.MEDICATION_CSV)\n",
    "\n",
    "    for ii,row in tqdm(new_filekeys.iterrows(), desc=\"Extracting raw data\", total=len(new_filekeys)):\n",
    "        filekey = row[Bucket.FILEKEY]\n",
    "        pattern = row[Bucket.PATTERN]\n",
    "        if print_filekey:\n",
    "            print(filekey)\n",
    "        if pattern in [\"UPDRS\", \"UPDRS3\", \"UPDRS124\"]:\n",
    "            if Qnnrs.UPDRS1.value not in row or pd.isna(row[Qnnrs.UPDRS1.value]) or row[Qnnrs.UPDRS1.value]=='':\n",
    "                df = download_csv_to_df(filekey)\n",
    "                new_filekeys.loc[ii, Qnnrs.UPDRS1] = df.loc[0, UPDRS.updrs1.value].astype(int).sum()\n",
    "                new_filekeys.loc[ii, Qnnrs.UPDRS2] = df.loc[0, UPDRS.updrs2.value].astype(int).sum()\n",
    "                new_filekeys.loc[ii, Qnnrs.UPDRS3] = df.loc[0, UPDRS.updrs3.value].astype(int).sum()\n",
    "                new_filekeys.loc[ii, Qnnrs.UPDRS4] = df.loc[0, UPDRS.updrs4.value].astype(int).sum()\n",
    "                new_filekeys.loc[ii, Qnnrs.HY] = df.loc[0, UPDRS.hy.value]\n",
    "                new_filekeys.loc[ii, Bucket.SAMPLER] = df.loc[0, Bucket.SAMPLER] if Bucket.SAMPLER in df else pd.NaT\n",
    "                if updrs.empty or (filekey not in updrs[Bucket.FILEKEY].tolist()):\n",
    "                    df[Bucket.FILEKEY] = filekey\n",
    "                    df = put_filekey_first(df)\n",
    "                    updrs = pd.concat([updrs, df], ignore_index=True)\n",
    "        \n",
    "        if pattern==\"MOCA\":\n",
    "            if Qnnrs.MOCA.value not in row or pd.isna(row[Qnnrs.MOCA.value]) or row[Qnnrs.MOCA.value]=='':\n",
    "                df = download_csv_to_df(filekey)\n",
    "                df = df.replace({\"True\": 1, \"False\": 0})\n",
    "                new_filekeys.loc[ii, Qnnrs.MOCA] = df.loc[0, MoCA.moca.value].astype(int).sum()\n",
    "                new_filekeys.loc[ii, Bucket.SAMPLER] = df.loc[0, Bucket.SAMPLER] if Bucket.SAMPLER in df else pd.NaT\n",
    "                if moca.empty or (filekey not in moca[Bucket.FILEKEY].tolist()):\n",
    "                    df[Bucket.FILEKEY] = filekey\n",
    "                    df = put_filekey_first(df)\n",
    "                    moca = pd.concat([moca, df], ignore_index=True)\n",
    "        \n",
    "        if pattern==\"PDQ8\":\n",
    "            if Qnnrs.PDQ8.value not in row or pd.isna(row[Qnnrs.PDQ8.value]) or row[Qnnrs.PDQ8.value]=='':\n",
    "                df = download_csv_to_df(filekey)\n",
    "                new_filekeys.loc[ii, Qnnrs.PDQ8] = df.loc[0, PDQ8.pdq8.value].astype(int).sum()\n",
    "                new_filekeys.loc[ii, Bucket.SAMPLER] = df.loc[0, Bucket.SAMPLER] if Bucket.SAMPLER in df else pd.NaT\n",
    "                if pdq8.empty or (filekey not in pdq8[Bucket.FILEKEY].tolist()):\n",
    "                    df[Bucket.FILEKEY] = filekey\n",
    "                    df = put_filekey_first(df)\n",
    "                    pdq8 = pd.concat([pdq8, df], ignore_index=True)\n",
    "        \n",
    "        if pattern==\"FOG\":\n",
    "            if Qnnrs.FOG.value not in row or pd.isna(row[Qnnrs.FOG.value]) or row[Qnnrs.FOG.value]=='':\n",
    "                df = download_csv_to_df(filekey)\n",
    "                new_filekeys.loc[ii, Qnnrs.FOG] = df.loc[0, FOG.fog.value].astype(int).sum()\n",
    "                if fog.empty or (filekey not in fog[Bucket.FILEKEY].tolist()):\n",
    "                    df[Bucket.FILEKEY] = filekey\n",
    "                    df = put_filekey_first(df)\n",
    "                    fog = pd.concat([fog, df], ignore_index=True)\n",
    "        \n",
    "        if pattern==\"SDQ\":\n",
    "            if Qnnrs.SDQ.value not in row or pd.isna(row[Qnnrs.SDQ.value]) or row[Qnnrs.SDQ.value]=='':\n",
    "                df = download_csv_to_df(filekey)\n",
    "                df = df.replace({\"True\": 1, \"False\": 0})\n",
    "                score = df.loc[0, SDQ.sdq.value[:-1]].astype(int).sum()\n",
    "                respiratory = 2.5 if df.loc[0, SDQ.sdq.value[-1]]==\"True\" else 0.5\n",
    "                score += respiratory\n",
    "                new_filekeys.loc[ii, Qnnrs.SDQ] = score\n",
    "                if sdq.empty or (filekey not in sdq[Bucket.FILEKEY].tolist()):\n",
    "                    df[Bucket.FILEKEY] = filekey\n",
    "                    df = put_filekey_first(df)\n",
    "                    sdq = pd.concat([sdq, df], ignore_index=True)\n",
    "        \n",
    "        if pattern==\"WOQ\":\n",
    "            if Qnnrs.WOQ_PRE.value not in row or pd.isna(row[Qnnrs.WOQ_PRE.value]) or row[Qnnrs.WOQ_PRE.value]=='':\n",
    "                df = download_csv_to_df(filekey)\n",
    "                df = df.replace({\"True\": 1, \"False\": 0})\n",
    "                try:\n",
    "                    new_filekeys.loc[ii, Qnnrs.WOQ_PRE] = df.loc[0, WOQ.pre.value].astype(int).sum()\n",
    "                    new_filekeys.loc[ii, Qnnrs.WOQ_POST] = df.loc[0, WOQ.pre.value].astype(int).sum() - df.loc[0, WOQ.post.value].astype(int).sum()\n",
    "                except:\n",
    "                    new_filekeys.loc[ii, Qnnrs.WOQ_PRE] = -1\n",
    "                    new_filekeys.loc[ii, Qnnrs.WOQ_POST] = -1\n",
    "                if woq.empty or (filekey not in woq[Bucket.FILEKEY].tolist()):\n",
    "                    df[Bucket.FILEKEY] = filekey\n",
    "                    df = put_filekey_first(df)\n",
    "                    woq = pd.concat([woq, df], ignore_index=True)\n",
    "\n",
    "        if pattern==\"REGISTRATION\":\n",
    "            if Registration.BIRTHDATE.value not in row or pd.isna(row[Registration.BIRTHDATE.value]) or row[Registration.BIRTHDATE.value]=='':\n",
    "                df = download_csv_to_df(filekey)\n",
    "                for col in Registration.values():\n",
    "                    if col in df:\n",
    "                        new_filekeys.loc[ii, col] = str(df.loc[0, col])\n",
    "                if registration.empty or (filekey not in registration[Bucket.FILEKEY].tolist()):\n",
    "                    df[Bucket.FILEKEY] = filekey\n",
    "                    df = put_filekey_first(df)\n",
    "                    registration = pd.concat([registration, df], ignore_index=True)\n",
    "    \n",
    "        if pattern==\"UPDATE\":\n",
    "            if Update.IS_DBS.value not in row or pd.isna(row[Update.IS_DBS.value]) or row[Update.IS_DBS.value]=='':\n",
    "                df = download_csv_to_df(filekey)\n",
    "                for col in Update.values():\n",
    "                    if col in df:\n",
    "                        new_filekeys.loc[ii, col] = str(df.loc[0, col])\n",
    "                if update.empty or (filekey not in update[Bucket.FILEKEY].tolist()):\n",
    "                    df = download_csv_to_df(filekey)\n",
    "                    df[Bucket.FILEKEY] = filekey\n",
    "                    df = put_filekey_first(df)\n",
    "                    update = pd.concat([update, df], ignore_index=True)\n",
    "\n",
    "        if pattern==\"MEDICATIONS\":\n",
    "            if medications.empty or (filekey not in medications[Bucket.FILEKEY].tolist()):\n",
    "                df = download_csv_to_df(filekey)\n",
    "                df[Bucket.FILEKEY] = filekey\n",
    "                df = put_filekey_first(df)\n",
    "                medications = pd.concat([medications, df.astype(str)], ignore_index=True)\n",
    "\n",
    "        \n",
    "    updrs.to_csv(Settings.UPDRS_CSV, index=False)\n",
    "    moca.to_csv(Settings.MOCA_CSV, index=False)\n",
    "    pdq8.to_csv(Settings.PDQ8_CSV, index=False)\n",
    "    fog.to_csv(Settings.FOG_CSV, index=False)\n",
    "    sdq.to_csv(Settings.SDQ_CSV, index=False)\n",
    "    woq.to_csv(Settings.WOQ_CSV, index=False)\n",
    "    update.to_csv(Settings.UPDATE_CSV, index=False)\n",
    "    registration.to_csv(Settings.REGISTRATION_CSV, index=False)\n",
    "    medications.to_csv(Settings.MEDICATION_CSV, index=False)\n",
    "\n",
    "    raw = pd.concat([raw, new_filekeys], ignore_index=True)\n",
    "    raw = raw.sort_values(by=['date', 'username', 'time'], ascending=False, ignore_index=True)\n",
    "    raw.to_csv(Settings.RAW_CSV, index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Get all files\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `change_columns()`: update values of attributes (columns) for specific filekeys.\n",
    "* `resolve()`: remove unneeded filekeys, update attributes for old patterns or filenames.\n",
    "* `add_patient_phone()`: merge sampler's name (from the registration csv) with samplers phones csv.\n",
    "* `add_patient_phone()`: use the data from the VM to merge the phone numbers of the patients (users).\n",
    "* `add_session_to_all()`: find the closest session (in time) for the sampler's files.\n",
    "* `resolve_sessions()`: resolve sessions issues; mostly merging sessions.\n",
    "* `propagate_values()`: propagate the data from the csv files to all filekeys.\n",
    "* Create `all_files.csv`, `sessions.csv`, and `all_users.csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_all_files():\n",
    "    print(\"Arranging data ...\", end=' ')\n",
    "    df = pd.read_csv(Settings.RAW_CSV, dtype=str)\n",
    "    df = change_columns(df)\n",
    "    df = resolve(df)\n",
    "\n",
    "    df = add_sampler_phone(df)\n",
    "    df = add_patient_phone(df)\n",
    "    df = add_caregiver_phone(df)\n",
    "\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], format=Settings.DATETIME, errors='coerce')\n",
    "    df = add_session_to_all(df)\n",
    "\n",
    "    real_sessions = df.copy()\n",
    "    real_sessions = add_session_number(real_sessions)\n",
    "    real_sessions = add_sampler_to_HC(real_sessions)\n",
    "\n",
    "    df = resolve_sessions(df)\n",
    "    df = add_session_number(df)\n",
    "    df = remove_qnnrs_duplicates(df)\n",
    "    df = add_sampler_to_HC(df)\n",
    "    print(\"Done!\")\n",
    "\n",
    "    df = propagate_values(df)\n",
    "    df = add_age(df)\n",
    "    df = add_updrs_columns(df)\n",
    "    df = df.sort_values(by=['date', 'username', 'time'], ascending=False)\n",
    "    df.to_csv(Settings.ALL_FILES, index=False)\n",
    "    \n",
    "    sessions = get_sessions(df)\n",
    "    sessions.to_csv(Settings.SESSIONS, index=False)\n",
    "    real_sessions = get_sessions(real_sessions)\n",
    "    real_sessions.to_csv(Settings.REAL_SESSIONS, index=False)\n",
    "\n",
    "    all_users = get_all_users(df)\n",
    "    all_users.to_csv(Settings.ALL_USERS, index=False)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pie plots\n",
    "![](pies.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def plot_pies(show_ax=False, exclude_archive=True, only_full=True, counter='time'):\n",
    "    users = pd.read_csv(Settings.ALL_USERS, dtype=str)\n",
    "    patients = users[users.entity=='PD']\n",
    "    healthy = users[users.entity=='HC']\n",
    "    # if exclude_archive:\n",
    "    #     healthy = healthy[~healthy.username.isin(IgnoreUsers.archive)]\n",
    "\n",
    "    npm = len(patients.loc[patients.gender == 'male', 'username'].unique())\n",
    "    npf = len(patients.loc[patients.gender == 'female', 'username'].unique())\n",
    "    nhm = len(healthy.loc[healthy.gender == 'male', 'username'].unique())\n",
    "    nhf = len(healthy.loc[healthy.gender == 'female', 'username'].unique())\n",
    "\n",
    "    sessions = pd.read_csv(Settings.SESSIONS, dtype=str)\n",
    "    # sessions = sessions[~sessions.username.isin(Exclude.users)]\n",
    "    if only_full:\n",
    "        sessions = sessions[~sessions['paradigm'].str.contains(\"~\")]\n",
    "    s_pd = len(sessions[(sessions.entity == 'PD')])\n",
    "    s_hc = len(sessions[(sessions.entity == 'HC')])\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 14))\n",
    "\n",
    "    labels = ['PD men', 'PD women', 'HC men', 'HC women']\n",
    "    sizes = [npm, npf, nhm, nhf]\n",
    "    colors = [Color.VV, Color.VV, Color.YELLOW, Color.YELLOW]\n",
    "    explode = [0.03, 0.03, 0.03, 0.03]\n",
    "\n",
    "    if show_ax:\n",
    "        ataxia = users[users.entity=='AX']\n",
    "        na = len(ataxia.username.unique())\n",
    "        labels.append('Ataxia')\n",
    "        sizes.append(na)\n",
    "        colors.append(Color.LIGHT_GREEN)\n",
    "        explode.append(0.05)\n",
    "\n",
    "    wedges, texts, autotexts = axs[0, 0].pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "                                             autopct=lambda p: f'{round(p * sum(sizes) / 100)}', shadow=True,\n",
    "                                             startangle=140)\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontsize(18)\n",
    "    axs[0, 0].set_title('Participants')\n",
    "\n",
    "\n",
    "\n",
    "    if counter=='number':\n",
    "        labels = ['PD', 'HC']\n",
    "        sizes = [s_pd * 46, s_hc * 23]\n",
    "        for ii in range(len(labels)):\n",
    "            labels[ii] = f\"{labels[ii]}: {sizes[ii] // 23}\"\n",
    "    elif counter=='time':\n",
    "        files = pd.read_csv(Settings.ALL_FILES, dtype=str)\n",
    "        files = files[files['pattern'].isin(['RECORDING','RECORDING1'])]\n",
    "        files['duration'] = files['exercise'].apply(lambda k: Durations[k])\n",
    "        labels = ['PD', 'HC']\n",
    "        sizePD = files.loc[files['entity']=='PD', 'duration'].sum()//60\n",
    "        sizeHC = files.loc[files['entity']=='HC', 'duration'].sum()//60\n",
    "        sizes = [sizePD, sizeHC]\n",
    "\n",
    "    colors = [Color.VV, Color.YELLOW]\n",
    "    explode = (0.03, 0.03)\n",
    "\n",
    "    wedges, texts, autotexts = axs[0, 1].pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "                                             autopct=lambda p: f'{round(p * sum(sizes) / 100)}\\nminutes', shadow=True,\n",
    "                                             startangle=140)\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontsize(18)\n",
    "        # autotext.set_color('white')\n",
    "    axs[0, 1].set_title('Recordings')\n",
    "\n",
    "\n",
    "\n",
    "    n_sheba = len(patients.loc[patients.medical_center == 'Sheba', 'username'].unique())\n",
    "    n_ichilov = len(patients.loc[patients.medical_center == 'Ichilov', 'username'].unique())\n",
    "\n",
    "    labels = ['Sheba', 'Ichilov']\n",
    "    sizes = [n_sheba, n_ichilov]\n",
    "    colors = [Color.DARK_VV, Color.GREEN]\n",
    "    explode = (0.03, 0.03)\n",
    "\n",
    "    wedges, texts, autotexts = axs[1, 0].pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "                                             autopct=lambda p: f'{round(p * sum(sizes) / 100)}', shadow=True,\n",
    "                                             startangle=140)\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontsize(18)\n",
    "    axs[1, 0].set_title('Medical Centers')\n",
    "\n",
    "    n_no = len(patients.loc[patients.genetic == 'NO', 'username'].unique())\n",
    "    n_yes = len(patients.loc[patients.genetic == 'YES', 'username'].unique())\n",
    "    n_gba = len(patients.loc[patients.genetic == 'GBA', 'username'].unique())\n",
    "    n_lrrk2 = len(patients.loc[patients.genetic == 'LRRK2', 'username'].unique())\n",
    "\n",
    "    labels = ['No', 'Yes', 'GBA', 'LRRK2']\n",
    "    sizes = [n_no, n_yes, n_gba, n_lrrk2]\n",
    "    colors = [Color.VV, Color.LIGHT_GREEN, Color.YELLOW, Color.GREEN]\n",
    "    explode = (0.03, 0.03, 0.03, 0.03)\n",
    "\n",
    "    wedges, texts, autotexts = axs[1, 1].pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "                                             autopct=lambda p: f'{round(p * sum(sizes) / 100)}', shadow=True,\n",
    "                                             startangle=140)\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontsize(18)\n",
    "    axs[1, 1].set_title('Genetic predisposition')\n",
    "\n",
    "    return fig, axs\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users over time\n",
    "![](overtime.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def plot_users_over_time(interval=None, exclude_archive=True, dropout=False, annotation_percentage=20):\n",
    "    users = pd.read_csv(Settings.ALL_USERS, dtype=str)\n",
    "    patients = users[users.entity == 'PD']\n",
    "    healthy = users[users.entity == 'HC']\n",
    "    \n",
    "    # if exclude_archive:\n",
    "    #     healthy = healthy[~healthy.username.isin(IgnoreUsers.archive)]\n",
    "\n",
    "    patients['date'] = pd.to_datetime(patients['date'])\n",
    "    healthy['date'] = pd.to_datetime(healthy['date'])\n",
    "\n",
    "    if dropout:\n",
    "        dropout_df = pd.read_csv(Settings.DROPOUT, dtype=str)\n",
    "        dropout_df['drop_out'] = pd.to_datetime(dropout_df['drop_out'], format=Settings.DATE_FORMAT)\n",
    "\n",
    "    patients_counts = patients.groupby(patients['date'].dt.date).size().cumsum()\n",
    "    healthy_counts = healthy.groupby(healthy['date'].dt.date).size().cumsum()\n",
    "\n",
    "    combined_counts_patients = pd.DataFrame({'Patients': patients_counts})\n",
    "    combined_counts_healthy = pd.DataFrame({'Healthy': healthy_counts})\n",
    "\n",
    "    if dropout:\n",
    "        # Convert all relevant dates to the same type (Timestamp)\n",
    "        all_dates = pd.date_range(start=min(combined_counts_patients.index.min(), dropout_df['drop_out'].min().date()), \n",
    "                                  end=max(combined_counts_patients.index.max(), dropout_df['drop_out'].max().date()))\n",
    "        combined_counts_patients = combined_counts_patients.reindex(all_dates, method='ffill').fillna(0)\n",
    "        combined_counts_healthy = combined_counts_healthy.reindex(all_dates, method='ffill').fillna(0)\n",
    "\n",
    "        for _, row in dropout_df.iterrows():\n",
    "            username = hashp(row['user_phone'])\n",
    "            if username in patients.username.values:\n",
    "                drop_date = row['drop_out'].date()\n",
    "                combined_counts_patients.loc[drop_date:] -= 1\n",
    "            elif username in healthy.username.values:\n",
    "                drop_date = row['drop_out'].date()\n",
    "                combined_counts_healthy.loc[drop_date:] -= 1\n",
    "\n",
    "    if interval:\n",
    "        start_date, end_date = pd.to_datetime(interval).date\n",
    "        combined_counts_patients = combined_counts_patients[(combined_counts_patients.index >= start_date) & (combined_counts_patients.index <= end_date)]\n",
    "        combined_counts_healthy = combined_counts_healthy[(combined_counts_healthy.index >= start_date) & (combined_counts_healthy.index <= end_date)]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(combined_counts_patients.index, combined_counts_patients['Patients'], color=Color.VV, label='Patients', marker='o', alpha=0.7)\n",
    "    ax.plot(combined_counts_healthy.index, combined_counts_healthy['Healthy'], color=Color.YELLOW, label='Healthy', marker='o', alpha=0.7)\n",
    "\n",
    "    fig.autofmt_xdate()\n",
    "\n",
    "    step_patients = max(1, len(combined_counts_patients) * annotation_percentage // 100)\n",
    "    step_healthy = max(1, len(combined_counts_healthy) * annotation_percentage // 100)\n",
    "\n",
    "    for i, (date, row) in enumerate(combined_counts_patients.iterrows()):\n",
    "        if i % step_patients == 0:\n",
    "            ax.annotate(f'{row[\"Patients\"]:.0f}', (date, row['Patients']), textcoords=\"offset points\", xytext=(0, 10), ha='center', color=Color.VV)\n",
    "\n",
    "    for i, (date, row) in enumerate(combined_counts_healthy.iterrows()):\n",
    "        if i % step_healthy == 0:\n",
    "            ax.annotate(f'{row[\"Healthy\"]:.0f}', (date, row['Healthy']), textcoords=\"offset points\", xytext=(0, 10), ha='center', color=Color.YELLOW)\n",
    "\n",
    "    upper_limit = 1.2 * max(combined_counts_healthy.max().max(), combined_counts_patients.max().max())\n",
    "    ax.set_ylim([0, upper_limit])\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Cumulative Number of Users')\n",
    "    ax.set_title('Number of Users Joined Over Time')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True)\n",
    "    return fig, ax\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users per sampler\n",
    "![How many users (PD/HC) each sampler sampled?](userpersampler.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def users_per_sampler(exclude_archive=True, show=None, ignore=[]):\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    users = pd.read_csv(Settings.ALL_USERS, dtype=str)\n",
    "    users['date'] = pd.to_datetime(users['date'])\n",
    "    users['month'] = users['date'].dt.to_period('M')\n",
    "    patients = users[users.entity == 'PD']\n",
    "    healthy = users[users.entity == 'HC']\n",
    "    if exclude_archive:\n",
    "        healthy = healthy[~healthy.username.isin(SpecialUsers.values())]\n",
    "\n",
    "    samplers = pd.read_csv(Settings.SAMPLERS_CSV, dtype=str)\n",
    "    if show:\n",
    "        samplers = samplers.loc[samplers.group == show, 'sampler_username']\n",
    "        patients = patients[patients['sampler_username'].isin(samplers)]\n",
    "        healthy = healthy[healthy['sampler_username'].isin(samplers)]\n",
    "\n",
    "    if ignore:\n",
    "        patients = patients[~patients['sampler_username'].isin(ignore)]\n",
    "        healthy = healthy[~healthy['sampler_username'].isin(ignore)]\n",
    "    \n",
    "    patient_counts = patients.groupby(['Hebrew', 'month'])['username'].nunique().unstack(fill_value=0)\n",
    "    healthy_counts = healthy.groupby(['Hebrew', 'month'])['username'].nunique().unstack(fill_value=0)\n",
    "    patient_cumulative = patient_counts.cumsum(axis=1)\n",
    "    patient_cumulative = patient_cumulative.mask(patient_cumulative.diff(axis=1) == 0)\n",
    "\n",
    "    combined_counts = pd.DataFrame({\n",
    "        'Patients': patient_counts.sum(axis=1),\n",
    "        'Healthy': healthy_counts.sum(axis=1)\n",
    "    }).fillna(0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    width = 0.4\n",
    "\n",
    "    combined_counts['Patients'].plot(kind='bar', color=Color.VV, width=width, position=1, label='Patients', ax=ax)\n",
    "    combined_counts['Healthy'].plot(kind='bar', color=Color.YELLOW, width=width, position=0, label='Healthy', ax=ax)\n",
    "\n",
    "    # Add white horizontal lines for monthly registrations in the PD bars, and month codes below them\n",
    "    for i, (sampler, row) in enumerate(patient_cumulative.iterrows()):\n",
    "        for month in range(1, len(row)):\n",
    "            y_position = row.iloc[month-1]\n",
    "            # Draw the white line if it's not at the top or bottom\n",
    "            if y_position > 0: # and y_position < row.iloc[-1]:\n",
    "                # Draw the white line only on the PD bar\n",
    "                ax.plot([i - width, i], [y_position]*2, color='white', linewidth=1.5)\n",
    "                # Add the month code below the white line\n",
    "                month_code = row.index[month-1].strftime('%b')\n",
    "                ax.text(i - width/2, y_position - 0.3, month_code, ha='center', color='white', va='top', fontsize=8)\n",
    "\n",
    "    ax.set_xlabel('Sampler')\n",
    "    ax.set_ylabel('Number of Users')\n",
    "    if show:\n",
    "        ax.set_title(f'Number of Users per Sampler - {show}')\n",
    "    else:\n",
    "        ax.set_title('Number of Users per Sampler')\n",
    "    ax.set_xticklabels([label.get_text()[::-1] for label in ax.get_xticklabels()], rotation=45)\n",
    "\n",
    "    for i, (patients, healthy) in enumerate(zip(combined_counts['Patients'], combined_counts['Healthy'])):\n",
    "        ax.text(i - width/2, patients + 1.0, f'{patients:.0f}', ha='center', color='black', va='bottom', fontsize=12)\n",
    "        ax.text(i + width/2, healthy + 1.0, f'{healthy:.0f}', ha='center', color='black', va='bottom', fontsize=12)\n",
    "\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y')\n",
    "    ax.set_xlim([-1, len(combined_counts)])\n",
    "    ax.set_ylim([0, 1.2 * combined_counts.max().max()])\n",
    "\n",
    "    return fig, ax\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of sessions vs. session number\n",
    "![](sessions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def plot_sessions_count(n, show_all=True, exclude_archive=True):\n",
    "    sessions = pd.read_csv(Settings.SESSIONS, dtype=str)\n",
    "    if exclude_archive:\n",
    "        sessions = sessions[~sessions.username.isin(SpecialUsers.values())]\n",
    "    sessions = sessions[(sessions['entity'] != 'AX')]\n",
    "\n",
    "    all_table = pd.DataFrame(index=range(1, n+1), columns=sessions['entity'].unique())\n",
    "    for i in all_table.index:\n",
    "        for entity in all_table.columns:\n",
    "            count = sessions[(sessions['entity'] == entity) & (sessions['session_number'].astype(int) >= i)]['username'].nunique()\n",
    "            all_table.at[i, entity] = count\n",
    "\n",
    "    full_sessions = sessions[(~sessions.paradigm.str.contains(\"~\"))]\n",
    "    full_table = pd.DataFrame(index=range(1, n+1), columns=full_sessions['entity'].unique())\n",
    "    for i in full_table.index:\n",
    "        for entity in full_table.columns:\n",
    "            count = full_sessions[(full_sessions['entity'] == entity) & (full_sessions['session_number'].astype(int) >= i)]['username'].nunique()\n",
    "            full_table.at[i, entity] = count\n",
    "\n",
    "    # Load dropout data\n",
    "    dropout = pd.read_csv(Settings.DROPOUT, dtype=str)\n",
    "    first_bar_value = all_table['PD'].iloc[0]\n",
    "    active_users = first_bar_value - len(dropout)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    bar_width = 0.35\n",
    "    indices = range(len(all_table.index))\n",
    "\n",
    "    if show_all:\n",
    "        ax.bar([i - bar_width/2 for i in indices], all_table['PD'], bar_width, label='All PD', color=Color.VV, alpha=0.5)\n",
    "        ax.bar([i + bar_width/2 for i in indices], all_table['HC'], bar_width, label='All HC', color=Color.YELLOW, alpha=0.5)\n",
    "\n",
    "    ax.bar([i - bar_width/2 for i in indices], full_table['PD'], bar_width, label='Full PD', color=Color.VV)\n",
    "    ax.bar([i + bar_width/2 for i in indices], full_table['HC'], bar_width, label='Full HC', color=Color.YELLOW)\n",
    "\n",
    "    if show_all:\n",
    "        for i, v in enumerate(all_table['PD']):\n",
    "            if v == 0:\n",
    "                continue\n",
    "            ax.text(i - bar_width/2, v + 4, str(v), ha='center', va='top', color='black', fontsize=12)\n",
    "        for i, v in enumerate(all_table['HC']):\n",
    "            if v == 0:\n",
    "                continue\n",
    "            ax.text(i + bar_width/2, v + 4, str(v), ha='center', va='top', color='black', fontsize=12)\n",
    "\n",
    "    for i, v in enumerate(full_table['PD']):\n",
    "        if v == 0:\n",
    "            continue\n",
    "        ax.text(i - bar_width/2, v - 4, str(v), ha='center', va='bottom', color='white', fontsize=12)\n",
    "    for i, v in enumerate(full_table['HC']):\n",
    "        if v == 0:\n",
    "            continue\n",
    "        ax.text(i + bar_width/2, v - 4, str(v), ha='center', va='bottom', color='white', fontsize=12)\n",
    "\n",
    "    ax.axhline(y=active_users, color=Color.GREEN, linewidth=1, linestyle='--')\n",
    "    ax.set_yticks([active_users])\n",
    "    # ax.set_yticklabels([f'Active Users: {active_users}'], fontsize=12, color='pink')\n",
    "\n",
    "    ax.set_xlabel('Number of Sessions')\n",
    "    ax.set_ylabel('Number of Users')\n",
    "    ax.set_title(f'Number of Users with up to {n} Sessions')\n",
    "    ax.legend(title='Entity')\n",
    "    ax.grid(axis='y')\n",
    "    ax.set_xticks(indices)\n",
    "    ax.set_xticklabels(all_table.index, rotation=0)\n",
    "    \n",
    "    return fig, ax\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questionnaires results\n",
    "![](qnnrs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def plot_histograms(bins=16):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(Settings.ALL_FILES, dtype=str)\n",
    "    exercises_order = ['updrs3', 'updrs124', 'moca', 'fog', 'sdq', 'woq', 'pdq8']\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 18))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Hide all axes initially\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    for idx, exercise in enumerate(exercises_order):\n",
    "        exercise_data = data[data['exercise'] == exercise]\n",
    "        \n",
    "        if exercise == 'updrs3':\n",
    "            pre_scores = exercise_data[exercise_data['timing'] == 'pre']['updrs3_pre'].dropna().astype(float)\n",
    "            post_scores = exercise_data[exercise_data['timing'] == 'post']['updrs3_post'].dropna().astype(float)\n",
    "            \n",
    "            axes[idx].hist(pre_scores, bins=np.linspace(0,132,bins[idx]) if isinstance(bins, list) else bins, alpha=0.5, label='Pre', color=Color.VV)\n",
    "            axes[idx].hist(post_scores, bins=np.linspace(0,132,bins[idx]) if isinstance(bins, list) else bins, alpha=0.5, label='Post', color=Color.YELLOW)\n",
    "            axes[idx].set_xlabel('Score')\n",
    "            axes[idx].set_ylabel('Frequency')\n",
    "            axes[idx].set_title('UPDRS3 Scores')\n",
    "            axes[idx].legend(loc='upper right')\n",
    "        \n",
    "        elif exercise == 'updrs124':\n",
    "            scores = exercise_data[['updrs1', 'updrs2', 'updrs4']].dropna().astype(float)\n",
    "            summed_scores = scores.apply(sum,1)\n",
    "            \n",
    "            axes[idx].hist(summed_scores, bins=bins[idx] if isinstance(bins, list) else bins, color=Color.GREEN)\n",
    "            axes[idx].set_xlabel('Summed Score')\n",
    "            axes[idx].set_ylabel('Frequency')\n",
    "            axes[idx].set_title('UPDRS124 Summed Scores')\n",
    "        \n",
    "        elif exercise == 'woq':\n",
    "            pre_scores = exercise_data['woq_pre'].dropna().astype(float)\n",
    "            post_scores = exercise_data['woq_post'].dropna().astype(float)\n",
    "                        \n",
    "            axes[idx].hist(pre_scores, bins=bins[idx] if isinstance(bins, list) else bins, alpha=0.5, label='Pre', color=Color.VV)\n",
    "            axes[idx].hist(post_scores, bins=bins[idx] if isinstance(bins, list) else bins, alpha=0.5, label='Post', color=Color.YELLOW)\n",
    "            axes[idx].set_xlabel('Score')\n",
    "            axes[idx].set_ylabel('Frequency')\n",
    "            axes[idx].set_title('WOQ Scores')\n",
    "            axes[idx].legend(loc='upper right')\n",
    "        \n",
    "        else:\n",
    "            scores = exercise_data[exercise].dropna().apply(pd.to_numeric, errors='coerce').dropna()\n",
    "            \n",
    "            axes[idx].hist(scores, bins=bins[idx] if isinstance(bins, list) else bins, color=Color.GREEN)\n",
    "            axes[idx].set_xlabel('Score')\n",
    "            axes[idx].set_ylabel('Frequency')\n",
    "            axes[idx].set_title(f'{exercise.upper()} Scores')\n",
    "        \n",
    "        axes[idx].axis('on')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    return fig, axes\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncompleted session (broken sessions)\n",
    "![](broken.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def broken_sessions(timeframe=60, only_sampler=False, ignore_sessions=True, fontsize=12, scale=(1.5, 3.1), figsize=(20, 10)):\n",
    "    df = pd.read_csv(Settings.SESSIONS, dtype=str)\n",
    "    if ignore_sessions:        \n",
    "        resolved = pd.read_csv(Settings.RESOLVED_SESSIONS, dtype=str)\n",
    "        filtered_sessions_df = df.merge(\n",
    "        resolved[['username', 'session']],\n",
    "        on=['username', 'session'],\n",
    "        how='left',\n",
    "        indicator=True\n",
    "        )\n",
    "        filtered_sessions_df = filtered_sessions_df[filtered_sessions_df['_merge'] == 'left_only']\n",
    "        df = filtered_sessions_df.drop(columns=['_merge'])\n",
    "        \n",
    "        dropout = pd.read_csv(Settings.DROPOUT, dtype=str)\n",
    "        dropout['username'] = dropout['user_phone'].apply(hashp)\n",
    "        dropout = dropout['username'].tolist()\n",
    "        df = df[~df['username'].isin(dropout)].reset_index(drop=True)\n",
    "\n",
    "    samplers = pd.read_csv(Settings.SAMPLERS_CSV, dtype=str, usecols=['sampler_username', 'Hebrew'])\n",
    "\n",
    "    # Merge samplers dataframe with sessions dataframe\n",
    "    df = df.merge(samplers, on='sampler_username', how='left')\n",
    "    df.rename(columns={'Hebrew': 'sampler'}, inplace=True)\n",
    "    # df.rename(columns={'sampler_username': 'sampler'}, inplace=True)\n",
    "    df['sampler'] = df['sampler'].fillna(df['session_number'])\n",
    "\n",
    "    broken_sessions = df.loc[df['paradigm'].str.contains(\"~\"), ['date', 'username', 'session', 'user_phone', 'start', 'end', 'sampler', 'paradigm']]\n",
    "    broken_sessions.to_csv(Settings.BROKEN_SESSIONS, index=False)\n",
    "    \n",
    "    # Show only last 30 days\n",
    "    broken_sessions['date'] = pd.to_datetime(broken_sessions['date'], format=Settings.DATE_FORMAT)\n",
    "    days_ago = datetime.now() - timedelta(days=timeframe)\n",
    "    broken_sessions = broken_sessions.loc[broken_sessions['date'] >= days_ago]\n",
    "    broken_sessions['date'] = broken_sessions['date'].dt.strftime(Settings.DATE_FORMAT)\n",
    "    \n",
    "    # Ensure columns are strings before checking for \"~\"\n",
    "    broken_sessions['paradigm'] = broken_sessions['paradigm'].astype(str).str.replace(\"'\",\"\")\n",
    "\n",
    "    # broken_sessions = df.loc[df['paradigm'].str.contains(\"~\"), ['date', 'username', 'session', 'user_phone', 'start', 'end', 'sampler', 'paradigm']]\n",
    "    # broken_sessions.to_csv(Settings.BROKEN_SESSIONS, index=False)\n",
    "\n",
    "    broken_sessions = broken_sessions[['date', 'user_phone', 'start', 'end', 'sampler', 'paradigm']]\n",
    "    broken_sessions['user_phone'] = broken_sessions['user_phone'].apply(lambda x: x[4:])\n",
    "    if only_sampler:\n",
    "        broken_sessions = broken_sessions[~broken_sessions['sampler'].str.isdigit()]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)  # Increase figure size for better fitting\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Escape special characters in column headers and data, and replace 'nan' with empty strings\n",
    "    broken_sessions.columns = [str(col).replace('$', r'\\$') for col in broken_sessions.columns]\n",
    "    broken_sessions = broken_sessions.fillna('')  # Replace NaN with empty strings\n",
    "    broken_sessions = broken_sessions.apply(lambda x: x.map(lambda y: str(y).replace('$', r'\\$')))\n",
    "    broken_sessions = broken_sessions.astype(str).map(lambda x: '' if x == 'nan' else x.replace('$', r'\\$'))\n",
    "\n",
    "    column_widths = {\n",
    "        'date': 0.085,\n",
    "        'user_phone': 0.09,\n",
    "        'start': 0.08,\n",
    "        'end': 0.08,\n",
    "        'sampler': 0.1,\n",
    "        'paradigm': 0.38,\n",
    "    }\n",
    "\n",
    "    # Create the table with some formatting\n",
    "    table = ax.table(cellText=broken_sessions.values, colLabels=broken_sessions.columns, cellLoc='center', loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(fontsize)\n",
    "    table.scale(*scale)  # Adjust scale to make sure columns are wide enough\n",
    "\n",
    "    cell_dict = table.get_celld()\n",
    "    # Adjust column widths based on the provided dictionary\n",
    "    for j, col in enumerate(broken_sessions.columns):\n",
    "        col_width = column_widths.get(col, 0.1)  # Default width if not specified in the dictionary\n",
    "        for i in range(len(broken_sessions) + 1):\n",
    "            cell_dict[(i, j)].set_width(col_width)\n",
    "\n",
    "    # Add colors to the table and handle special cases\n",
    "    for i in range(len(broken_sessions) + 1):\n",
    "        for j in range(len(broken_sessions.columns)):\n",
    "            cell_dict[(i, j)].set_edgecolor('black')\n",
    "            cell_dict[(i, j)].set_linewidth(1)\n",
    "            if i == 0:\n",
    "                cell_dict[(i, j)].set_facecolor('#40466e')\n",
    "                cell_dict[(i, j)].set_text_props(weight='bold', color='white')\n",
    "            else:\n",
    "                if '~' in cell_dict[(i, j)].get_text().get_text():\n",
    "                    cell_dict[(i, j)].set_facecolor('#bebfd4')  # Very light purple\n",
    "                else:\n",
    "                    cell_dict[(i, j)].set_facecolor('#f0f0f0' if i % 2 == 0 else '#e0e0e0')\n",
    "\n",
    "\n",
    "    # Here's where we apply the row border coloring\n",
    "    sampler_col_index = broken_sessions.columns.get_loc('sampler')\n",
    "    user_paradigm_col_index = broken_sessions.columns.get_loc('paradigm')\n",
    "    duplicate_phones = broken_sessions['user_phone'].duplicated(keep=False)\n",
    "\n",
    "    color_map = {\n",
    "                2: '#ff0000',     # Red\n",
    "                3: '#90ee90',     # Light green\n",
    "                4: '#ff00ff',     # Magenta\n",
    "                5: '#ffff00',     # Yellow\n",
    "                6: '#006400',     # Dark green\n",
    "                7: '#000000',     # Black\n",
    "                8: '#a52a2a',     # Brown\n",
    "                9: '#0000ff',     # Blue\n",
    "                10: '#ffa500'     # Orange\n",
    "            }\n",
    "    \n",
    "    for i in range(1, len(broken_sessions) + 1):\n",
    "        if broken_sessions.iloc[i - 1]['sampler'].isdigit():\n",
    "            sampler_value = int(broken_sessions.iloc[i - 1]['sampler'])\n",
    "            color = color_map.get(sampler_value, '#ffffff')  # Default to white if not in map\n",
    "\n",
    "            # Apply the color with half transparency\n",
    "            cell_dict[(i, sampler_col_index)].set_facecolor(color)\n",
    "            cell_dict[(i, sampler_col_index)].set_alpha(0.5)\n",
    "            # Color borders for 'sampler' column\n",
    "            # cell_dict[(i, sampler_col_index)].set_edgecolor('#3232ff')\n",
    "            # cell_dict[(i, sampler_col_index)].set_linewidth(4)\n",
    "            # Color borders for 'user_paradigm' column\n",
    "            # cell_dict[(i, user_paradigm_col_index)].set_edgecolor('#3232ff')\n",
    "            # cell_dict[(i, user_paradigm_col_index)].set_linewidth(4)\n",
    "\n",
    "        if duplicate_phones.iloc[i - 1]:\n",
    "        # Get the corresponding sampler value\n",
    "            sampler_value = broken_sessions.iloc[i - 1]['sampler']\n",
    "            if sampler_value.isdigit():\n",
    "                sampler_value = int(sampler_value)\n",
    "                color = color_map.get(sampler_value, '#ffffff')  # Default to white if not in map\n",
    "\n",
    "                # Apply the color with half transparency to the 'user_phone' cell\n",
    "                cell_dict[(i, broken_sessions.columns.get_loc('user_phone'))].set_facecolor(color)\n",
    "                cell_dict[(i, broken_sessions.columns.get_loc('user_phone'))].set_alpha(0.5)\n",
    "\n",
    "\n",
    "    # Ensure 'sampler' text is right-to-left\n",
    "    for i in range(1, len(broken_sessions) + 1):\n",
    "        for j in range(len(broken_sessions.columns)):\n",
    "            if broken_sessions.columns[j] == 'sampler':\n",
    "                cell_dict[(i, j)].get_text().set_text(' ' + cell_dict[(i, j)].get_text().get_text()[::-1])  # Reverse text for RTL effect\n",
    "\n",
    "    # Ensure 'user_paradigm' text is aligned to the left and uses a monospaced font\n",
    "    for i in range(1, len(broken_sessions) + 1):\n",
    "        for j in range(len(broken_sessions.columns)):\n",
    "            if broken_sessions.columns[j] in ['paradigm']:\n",
    "                cell_dict[(i, j)].get_text().set_ha('center')\n",
    "                cell_dict[(i, j)].get_text().set_fontproperties(font_manager.FontProperties(family='monospace', size=fontsize))\n",
    "    \n",
    "    return fig, ax\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users that need to record their next session\n",
    "![](password.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def plot_dataframe_table(source_file, X=28, fontsize=11, scale=1.3, figsize=(20, 24)):\n",
    "    \n",
    "    def send_recording_update_to(source_file=source_file, X=X):\n",
    "        with open(Settings.CONFIG, 'r') as file:\n",
    "            cfg = yaml.safe_load(file)\n",
    "\n",
    "        # Load session and dropout data\n",
    "        sessions_df = pd.read_csv(Settings.SESSIONS, dtype=str)\n",
    "        dropout = pd.read_csv(Settings.DROPOUT, dtype=str)\n",
    "        dropout['username'] = dropout['user_phone'].apply(hashp)\n",
    "        dropout = dropout['username'].tolist()\n",
    "        sessions_df = sessions_df[~sessions_df['username'].isin(dropout)].reset_index(drop=True)\n",
    "\n",
    "        # Filter by 'entity' == 'PD'\n",
    "        sessions_df = sessions_df[sessions_df['entity'] == 'PD']\n",
    "\n",
    "        # Convert 'date' to datetime for sorting and calculation purposes\n",
    "        sessions_df['date'] = pd.to_datetime(sessions_df['date'])\n",
    "\n",
    "        # Keep only the last session for each user\n",
    "        sessions_df = sessions_df.sort_values(by=['username', 'date'], ascending=[True, False])\n",
    "        sessions_df = sessions_df.drop_duplicates(subset='username', keep='first').reset_index(drop=True)\n",
    "        \n",
    "        # Rename 'date' to 'last session' and calculate 'days'\n",
    "        sessions_df['last session'] = sessions_df['date']\n",
    "        sessions_df['days'] = sessions_df['last session'].apply(lambda x: (datetime.now() - x).days)\n",
    "        \n",
    "        # Sort by 'days'\n",
    "        sessions_df = sessions_df.sort_values(by='days', ascending=False, ignore_index=True)\n",
    "        \n",
    "        # Add 'send?' column based on the X value\n",
    "        sessions_df['SMS'] = sessions_df['days'].apply(lambda x: 1 if x >= X else 0)\n",
    "        sessions_df = sessions_df[sessions_df['days'] >= X-3]\n",
    "        \n",
    "        # Check if the source file exists\n",
    "        if os.path.exists(source_file):\n",
    "            # Add columns for 'called', 'answered', and 'quit' from source_file\n",
    "            source_df = pd.read_csv(source_file, dtype=str)\n",
    "            \n",
    "            # Check if 'called', 'answered', and 'quit' exist in the source file, if not, set NaN\n",
    "            for col in cfg['gal_columns']:\n",
    "                if col not in source_df.columns:\n",
    "                    source_df[col] = np.nan\n",
    "            \n",
    "            # Merge source_df with sessions_df on 'username'\n",
    "            sessions_df = pd.merge(sessions_df, source_df[['username'] + cfg['gal_columns']], on='username', how='left')\n",
    "        else:\n",
    "            # If the source file doesn't exist, add NaN for 'called', 'answered', and 'quit'\n",
    "            sessions_df['called'] = np.nan\n",
    "            sessions_df['answered'] = np.nan\n",
    "            sessions_df['quit'] = np.nan\n",
    "        \n",
    "        # Reorder columns to include new ones\n",
    "        sessions_df = sessions_df[['username', 'user_phone', 'password', 'last session', 'days', 'session_number', 'SMS']+cfg['gal_columns']]\n",
    "        \n",
    "        return sessions_df\n",
    "\n",
    "\n",
    "    df = send_recording_update_to(source_file)  \n",
    "    # Replace 'NaN' with an empty string\n",
    "    df = df.fillna('')\n",
    "\n",
    "    # Adjust 'last session' to show only the date\n",
    "    if 'last session' in df.columns:\n",
    "        df['last session'] = pd.to_datetime(df['last session']).dt.date\n",
    "\n",
    "    # Export the DataFrame to an Excel file\n",
    "    df.to_csv(Settings.SEND_PASSWORDS, index=False)\n",
    "\n",
    "    df['comments'] = df['comments'].apply(lambda x: x[::-1] if isinstance(x, str) else x)\n",
    "    df.rename(columns={'session_number': 'number',\n",
    "                       'user_phone': 'phone'}, inplace=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)  # Increase figure size for better fitting\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Escape special characters in column headers and data\n",
    "    df.columns = [str(col).replace('$', r'\\$') for col in df.columns]\n",
    "    df = df.apply(lambda x: x.map(lambda y: str(y).replace('$', r'\\$')))\n",
    "    df['phone'] = df['phone'].apply(lambda x: x[4:])\n",
    "    # Create the table with some formatting\n",
    "    table = ax.table(cellText=df.values, colLabels=df.columns, cellLoc='center', loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(fontsize)\n",
    "    table.scale(scale, scale+0.5)  # Adjust scale to make sure columns are wide enough\n",
    "\n",
    "    # Format the 'username' column to be bold\n",
    "    cell_dict = table.get_celld()\n",
    "    for i in range(len(df)):\n",
    "        cell_dict[(i + 1, 0)].set_text_props(weight='bold')\n",
    "\n",
    "    # Adjust column widths\n",
    "    for j in range(len(df.columns)):\n",
    "        max_len = max([len(str(s)) for s in df.iloc[:, j]] + [len(df.columns[j])])\n",
    "        table.auto_set_column_width(j)\n",
    "        if max_len > 10:\n",
    "            table.auto_set_column_width(j)\n",
    "            for i in range(len(df) + 1):\n",
    "                cell_dict[(i, j)].set_width(0.15)\n",
    "\n",
    "    # Add colors to the table\n",
    "    for i in range(len(df) + 1):\n",
    "        for j in range(len(df.columns)):\n",
    "            cell_dict[(i, j)].set_edgecolor('black')\n",
    "            cell_dict[(i, j)].set_linewidth(1)\n",
    "            if i == 0:\n",
    "                cell_dict[(i, j)].set_facecolor('#40466e')\n",
    "                cell_dict[(i, j)].set_text_props(weight='bold', color='white')\n",
    "            else:\n",
    "                cell_dict[(i, j)].set_facecolor('#f0f0f0' if i % 2 == 0 else '#e0e0e0')\n",
    "\n",
    "    return fig, ax\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
